{"ast":null,"code":"\"use strict\";\n\nvar _objectSpread = require(\"F:\\\\programnodejs\\\\project\\\\client\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/objectSpread\");\nvar _classCallCheck = require(\"F:\\\\programnodejs\\\\project\\\\client\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/classCallCheck\");\nvar _createClass = require(\"F:\\\\programnodejs\\\\project\\\\client\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/createClass\");\nvar _possibleConstructorReturn = require(\"F:\\\\programnodejs\\\\project\\\\client\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/possibleConstructorReturn\");\nvar _getPrototypeOf = require(\"F:\\\\programnodejs\\\\project\\\\client\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/getPrototypeOf\");\nvar _inherits = require(\"F:\\\\programnodejs\\\\project\\\\client\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/inherits\");\nvar _assertThisInitialized = require(\"F:\\\\programnodejs\\\\project\\\\client\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/assertThisInitialized\");\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.GridFSBucketWriteStream = void 0;\nvar stream_1 = require(\"stream\");\nvar bson_1 = require(\"../bson\");\nvar error_1 = require(\"../error\");\nvar utils_1 = require(\"../utils\");\nvar write_concern_1 = require(\"./../write_concern\");\n/**\n * A writable stream that enables you to write buffers to GridFS.\n *\n * Do not instantiate this class directly. Use `openUploadStream()` instead.\n * @public\n */\nvar GridFSBucketWriteStream = /*#__PURE__*/function (_stream_1$Writable) {\n  _inherits(GridFSBucketWriteStream, _stream_1$Writable);\n  /** @internal\n   * @param bucket - Handle for this stream's corresponding bucket\n   * @param filename - The value of the 'filename' key in the files doc\n   * @param options - Optional settings.\n   */\n  function GridFSBucketWriteStream(bucket, filename, options) {\n    var _this;\n    _classCallCheck(this, GridFSBucketWriteStream);\n    _this = _possibleConstructorReturn(this, _getPrototypeOf(GridFSBucketWriteStream).call(this));\n    options = options !== null && options !== void 0 ? options : {};\n    _this.bucket = bucket;\n    _this.chunks = bucket.s._chunksCollection;\n    _this.filename = filename;\n    _this.files = bucket.s._filesCollection;\n    _this.options = options;\n    _this.writeConcern = write_concern_1.WriteConcern.fromOptions(options) || bucket.s.options.writeConcern;\n    // Signals the write is all done\n    _this.done = false;\n    _this.id = options.id ? options.id : new bson_1.ObjectId();\n    // properly inherit the default chunksize from parent\n    _this.chunkSizeBytes = options.chunkSizeBytes || _this.bucket.s.options.chunkSizeBytes;\n    _this.bufToStore = Buffer.alloc(_this.chunkSizeBytes);\n    _this.length = 0;\n    _this.n = 0;\n    _this.pos = 0;\n    _this.state = {\n      streamEnd: false,\n      outstandingRequests: 0,\n      errored: false,\n      aborted: false\n    };\n    if (!_this.bucket.s.calledOpenUploadStream) {\n      _this.bucket.s.calledOpenUploadStream = true;\n      checkIndexes(_assertThisInitialized(_assertThisInitialized(_this)), function () {\n        _this.bucket.s.checkedIndexes = true;\n        _this.bucket.emit('index');\n      });\n    }\n    return _this;\n  }\n  _createClass(GridFSBucketWriteStream, [{\n    key: \"write\",\n    value: function write(chunk, encodingOrCallback, callback) {\n      var _this2 = this;\n      var encoding = typeof encodingOrCallback === 'function' ? undefined : encodingOrCallback;\n      callback = typeof encodingOrCallback === 'function' ? encodingOrCallback : callback;\n      return waitForIndexes(this, function () {\n        return doWrite(_this2, chunk, encoding, callback);\n      });\n    }\n  }, {\n    key: \"abort\",\n    value: function abort(callback) {\n      var _this3 = this;\n      return (0, utils_1.maybePromise)(callback, function (callback) {\n        if (_this3.state.streamEnd) {\n          // TODO(NODE-3485): Replace with MongoGridFSStreamClosed\n          return callback(new error_1.MongoAPIError('Cannot abort a stream that has already completed'));\n        }\n        if (_this3.state.aborted) {\n          // TODO(NODE-3485): Replace with MongoGridFSStreamClosed\n          return callback(new error_1.MongoAPIError('Cannot call abort() on a stream twice'));\n        }\n        _this3.state.aborted = true;\n        _this3.chunks.deleteMany({\n          files_id: _this3.id\n        }, function (error) {\n          return callback(error);\n        });\n      });\n    }\n  }, {\n    key: \"end\",\n    value: function end(chunkOrCallback, encodingOrCallback, callback) {\n      var _this4 = this;\n      var chunk = typeof chunkOrCallback === 'function' ? undefined : chunkOrCallback;\n      var encoding = typeof encodingOrCallback === 'function' ? undefined : encodingOrCallback;\n      callback = typeof chunkOrCallback === 'function' ? chunkOrCallback : typeof encodingOrCallback === 'function' ? encodingOrCallback : callback;\n      if (this.state.streamEnd || checkAborted(this, callback)) return this;\n      this.state.streamEnd = true;\n      if (callback) {\n        this.once(GridFSBucketWriteStream.FINISH, function (result) {\n          if (callback) callback(undefined, result);\n        });\n      }\n      if (!chunk) {\n        waitForIndexes(this, function () {\n          return !!writeRemnant(_this4);\n        });\n        return this;\n      }\n      this.write(chunk, encoding, function () {\n        writeRemnant(_this4);\n      });\n      return this;\n    }\n  }]);\n  return GridFSBucketWriteStream;\n}(stream_1.Writable);\nexports.GridFSBucketWriteStream = GridFSBucketWriteStream;\n/** @event */\nGridFSBucketWriteStream.CLOSE = 'close';\n/** @event */\nGridFSBucketWriteStream.ERROR = 'error';\n/**\n * `end()` was called and the write stream successfully wrote the file metadata and all the chunks to MongoDB.\n * @event\n */\nGridFSBucketWriteStream.FINISH = 'finish';\nfunction __handleError(stream, error, callback) {\n  if (stream.state.errored) {\n    return;\n  }\n  stream.state.errored = true;\n  if (callback) {\n    return callback(error);\n  }\n  stream.emit(GridFSBucketWriteStream.ERROR, error);\n}\nfunction createChunkDoc(filesId, n, data) {\n  return {\n    _id: new bson_1.ObjectId(),\n    files_id: filesId,\n    n: n,\n    data: data\n  };\n}\nfunction checkChunksIndex(stream, callback) {\n  stream.chunks.listIndexes().toArray(function (error, indexes) {\n    var index;\n    if (error) {\n      // Collection doesn't exist so create index\n      if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {\n        index = {\n          files_id: 1,\n          n: 1\n        };\n        stream.chunks.createIndex(index, {\n          background: false,\n          unique: true\n        }, function (error) {\n          if (error) {\n            return callback(error);\n          }\n          callback();\n        });\n        return;\n      }\n      return callback(error);\n    }\n    var hasChunksIndex = false;\n    if (indexes) {\n      indexes.forEach(function (index) {\n        if (index.key) {\n          var keys = Object.keys(index.key);\n          if (keys.length === 2 && index.key.files_id === 1 && index.key.n === 1) {\n            hasChunksIndex = true;\n          }\n        }\n      });\n    }\n    if (hasChunksIndex) {\n      callback();\n    } else {\n      index = {\n        files_id: 1,\n        n: 1\n      };\n      var writeConcernOptions = getWriteOptions(stream);\n      stream.chunks.createIndex(index, _objectSpread({}, writeConcernOptions, {\n        background: true,\n        unique: true\n      }), callback);\n    }\n  });\n}\nfunction checkDone(stream, callback) {\n  if (stream.done) return true;\n  if (stream.state.streamEnd && stream.state.outstandingRequests === 0 && !stream.state.errored) {\n    // Set done so we do not trigger duplicate createFilesDoc\n    stream.done = true;\n    // Create a new files doc\n    var filesDoc = createFilesDoc(stream.id, stream.length, stream.chunkSizeBytes, stream.filename, stream.options.contentType, stream.options.aliases, stream.options.metadata);\n    if (checkAborted(stream, callback)) {\n      return false;\n    }\n    stream.files.insertOne(filesDoc, getWriteOptions(stream), function (error) {\n      if (error) {\n        return __handleError(stream, error, callback);\n      }\n      stream.emit(GridFSBucketWriteStream.FINISH, filesDoc);\n      stream.emit(GridFSBucketWriteStream.CLOSE);\n    });\n    return true;\n  }\n  return false;\n}\nfunction checkIndexes(stream, callback) {\n  stream.files.findOne({}, {\n    projection: {\n      _id: 1\n    }\n  }, function (error, doc) {\n    if (error) {\n      return callback(error);\n    }\n    if (doc) {\n      return callback();\n    }\n    stream.files.listIndexes().toArray(function (error, indexes) {\n      var index;\n      if (error) {\n        // Collection doesn't exist so create index\n        if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {\n          index = {\n            filename: 1,\n            uploadDate: 1\n          };\n          stream.files.createIndex(index, {\n            background: false\n          }, function (error) {\n            if (error) {\n              return callback(error);\n            }\n            checkChunksIndex(stream, callback);\n          });\n          return;\n        }\n        return callback(error);\n      }\n      var hasFileIndex = false;\n      if (indexes) {\n        indexes.forEach(function (index) {\n          var keys = Object.keys(index.key);\n          if (keys.length === 2 && index.key.filename === 1 && index.key.uploadDate === 1) {\n            hasFileIndex = true;\n          }\n        });\n      }\n      if (hasFileIndex) {\n        checkChunksIndex(stream, callback);\n      } else {\n        index = {\n          filename: 1,\n          uploadDate: 1\n        };\n        var writeConcernOptions = getWriteOptions(stream);\n        stream.files.createIndex(index, _objectSpread({}, writeConcernOptions, {\n          background: false\n        }), function (error) {\n          if (error) {\n            return callback(error);\n          }\n          checkChunksIndex(stream, callback);\n        });\n      }\n    });\n  });\n}\nfunction createFilesDoc(_id, length, chunkSize, filename, contentType, aliases, metadata) {\n  var ret = {\n    _id: _id,\n    length: length,\n    chunkSize: chunkSize,\n    uploadDate: new Date(),\n    filename: filename\n  };\n  if (contentType) {\n    ret.contentType = contentType;\n  }\n  if (aliases) {\n    ret.aliases = aliases;\n  }\n  if (metadata) {\n    ret.metadata = metadata;\n  }\n  return ret;\n}\nfunction doWrite(stream, chunk, encoding, callback) {\n  if (checkAborted(stream, callback)) {\n    return false;\n  }\n  var inputBuf = Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk, encoding);\n  stream.length += inputBuf.length;\n  // Input is small enough to fit in our buffer\n  if (stream.pos + inputBuf.length < stream.chunkSizeBytes) {\n    inputBuf.copy(stream.bufToStore, stream.pos);\n    stream.pos += inputBuf.length;\n    callback && callback();\n    // Note that we reverse the typical semantics of write's return value\n    // to be compatible with node's `.pipe()` function.\n    // True means client can keep writing.\n    return true;\n  }\n  // Otherwise, buffer is too big for current chunk, so we need to flush\n  // to MongoDB.\n  var inputBufRemaining = inputBuf.length;\n  var spaceRemaining = stream.chunkSizeBytes - stream.pos;\n  var numToCopy = Math.min(spaceRemaining, inputBuf.length);\n  var outstandingRequests = 0;\n  var _loop = function _loop() {\n    var inputBufPos = inputBuf.length - inputBufRemaining;\n    inputBuf.copy(stream.bufToStore, stream.pos, inputBufPos, inputBufPos + numToCopy);\n    stream.pos += numToCopy;\n    spaceRemaining -= numToCopy;\n    var doc = void 0;\n    if (spaceRemaining === 0) {\n      doc = createChunkDoc(stream.id, stream.n, Buffer.from(stream.bufToStore));\n      ++stream.state.outstandingRequests;\n      ++outstandingRequests;\n      if (checkAborted(stream, callback)) {\n        return {\n          v: false\n        };\n      }\n      stream.chunks.insertOne(doc, getWriteOptions(stream), function (error) {\n        if (error) {\n          return __handleError(stream, error);\n        }\n        --stream.state.outstandingRequests;\n        --outstandingRequests;\n        if (!outstandingRequests) {\n          stream.emit('drain', doc);\n          callback && callback();\n          checkDone(stream);\n        }\n      });\n      spaceRemaining = stream.chunkSizeBytes;\n      stream.pos = 0;\n      ++stream.n;\n    }\n    inputBufRemaining -= numToCopy;\n    numToCopy = Math.min(spaceRemaining, inputBufRemaining);\n  };\n  while (inputBufRemaining > 0) {\n    var _ret = _loop();\n    if (typeof _ret === \"object\") return _ret.v;\n  }\n  // Note that we reverse the typical semantics of write's return value\n  // to be compatible with node's `.pipe()` function.\n  // False means the client should wait for the 'drain' event.\n  return false;\n}\nfunction getWriteOptions(stream) {\n  var obj = {};\n  if (stream.writeConcern) {\n    obj.writeConcern = {\n      w: stream.writeConcern.w,\n      wtimeout: stream.writeConcern.wtimeout,\n      j: stream.writeConcern.j\n    };\n  }\n  return obj;\n}\nfunction waitForIndexes(stream, callback) {\n  if (stream.bucket.s.checkedIndexes) {\n    return callback(false);\n  }\n  stream.bucket.once('index', function () {\n    callback(true);\n  });\n  return true;\n}\nfunction writeRemnant(stream, callback) {\n  // Buffer is empty, so don't bother to insert\n  if (stream.pos === 0) {\n    return checkDone(stream, callback);\n  }\n  ++stream.state.outstandingRequests;\n  // Create a new buffer to make sure the buffer isn't bigger than it needs\n  // to be.\n  var remnant = Buffer.alloc(stream.pos);\n  stream.bufToStore.copy(remnant, 0, 0, stream.pos);\n  var doc = createChunkDoc(stream.id, stream.n, remnant);\n  // If the stream was aborted, do not write remnant\n  if (checkAborted(stream, callback)) {\n    return false;\n  }\n  stream.chunks.insertOne(doc, getWriteOptions(stream), function (error) {\n    if (error) {\n      return __handleError(stream, error);\n    }\n    --stream.state.outstandingRequests;\n    checkDone(stream);\n  });\n  return true;\n}\nfunction checkAborted(stream, callback) {\n  if (stream.state.aborted) {\n    if (typeof callback === 'function') {\n      // TODO(NODE-3485): Replace with MongoGridFSStreamClosedError\n      callback(new error_1.MongoAPIError('Stream has been aborted'));\n    }\n    return true;\n  }\n  return false;\n}","map":null,"metadata":{},"sourceType":"script"}